# 12 RAG Pain Points and Mitigation Strategies

| # | Pain Point | What that means / why it happens | Possible Solutions / Mitigations |
|---|---|---|---|
| **1.** | **Missing Content** | The information you need simply isn’t in the knowledge base. Or the content exists but it’s in unusable or “garbage” form (e.g. parser messed up, very messy source). | - Use good document ingestion / parsing pipelines. <br>- Preprocess documents to ensure quality (clean up scanned PDFs, images, bad OCR, etc.). <br>- Ensure frequent updating of the knowledge base if sources change over time. <br>- Add good metadata so content is easier to find. |
| **2.** | **Missed the Top Ranked Documents** | The retriever (or similarity / embedding / sparse + dense retrieval) doesn't return the relevant documents, even though they exist. So you query, but the relevant piece is ranked too low or missing. | - Hyperparameter tuning for retrieval: how many top-k you fetch; embedding model choice; embedding quality. <br>- Use reranking: fetch a larger set, then rerank more carefully. <br>- Use metadata filtering. <br>- Possibly fine-tune embedding models to domain data. |
| **3.** | **Not in Context — Consolidation Strategy Limitations** | Even if the document is retrieved, the way the document is chunked / how context is assembled might cause loss of important “global” context. Chunking strategy might break meaning. Also, context might be fragmented across chunks so no single chunk has the full picture. | - Better chunking / splitting strategies (by section, by logical boundaries, not just by fixed token counts). <br>- Include metadata linking, like which document / section each chunk is from. <br>- Possibly hierarchical approaches: consider larger context / parent document structure. <br>- Use traversal or node-hierarchy (e.g. document graph) to maintain structure. |
| **4.** | **Not Extracted** | The relevant content was present in a chunk / retrieved, but the LLM or the prompt didn’t properly extract that piece. Maybe it got lost, or the LLM overlooked it. Sometimes because context is too long, or because of model’s attention / “needle in haystack” problems. | - Use “needle in haystack” tests to see whether LLM can find small bits in large context. <br>- Reorder or prioritize retrieved chunks so the most relevant go earlier or at both ends. <br>- Use prompt compression or summarization to reduce irrelevant content. <br>- Monitor how much context you feed, maybe chunk and combine progressively. |
| **5.** | **Wrong Format** | The answer comes back in a format you didn’t want (e.g. prose when you wanted JSON, or wrong style, or wrong structure). Even if content is correct, its structure is unacceptable. | - Use prompt engineering to enforce format (e.g. “Answer in JSON with fields X, Y, Z”). <br>- Use function calling / structured output APIs. <br>- Use templates or schemas. <br>- Use validation / post-processing to check output format, possibly correct it. |
| **6.** | **Incorrect Specificity** | The answer is either too generic or too detailed, or misses the right level of specificity. For example: user asks “tell me about X in context Y”, but answer ignores Y or is too broad. | - Clarify in prompt what level of detail is needed. <br>- Use metadata or context filters that narrow to the needed subdomain. <br>- Use retrieval weighting or reranking to favor more precise / specific docs. <br>- Possibly incorporate user feedback or examples. |
| **7.** | **Incomplete** | The answer doesn’t cover all the parts of a multi-part question. Might miss steps, or leave gaps. | - Break down complex questions into sub-questions; use “agentic” planning / multi-step pipelines. <br>- Use query decomposition. <br>- Ensure retrieval fetches enough context (increase top-k, ensure coverage). <br>- Possibly iterate: answer, then ask “is that everything?”, then supplement. |
| **8.** | **Data Ingestion Scalability** | As your dataset grows (more documents, new updates), ingestion, indexing, embedding, updating vector databases become resource / time / cost heavy. Also stale data issues. | - Build pipelines to handle incremental updates (don’t reindex everything every time). <br>- Use efficient vector database / indexing systems. <br>- Monitor resource usage / latency; optimize embedding speed. <br>- Possibly prune old or less relevant documents; compress data. |
| **9.** | **Structured Data QA** | When the data is structured (tables, spreadsheets, etc.), naïve RAG (text chunking) often fails: losing spatial structure, misparsing rows/columns etc. Questions over structured data become error-prone. | - Use specialized parsers / table extraction tools. <br>- Preserve spatial information. <br>- Represent structured data separately (table nodes) in the ingestion/indexing. <br>- Possibly summarize structured data into text + retain schema. <br>- Use hybrid retrieval methods that understand structured vs unstructured. |
| **10.** | **Data Extraction from Complex PDFs** | PDFs are notoriously messy: they may have poor layout, multiple columns, embedded tables, images, inconsistent formatting. Chunking may break across logical sections, lose headings, etc. | - Use advanced PDF parsers that handle layout, tables, images. <br>- Detect sections/headings; chunk by section. <br>- Extract tables and graphs separately. <br>- Build hierarchical document graph: text nodes + table/image nodes. <br>- QA over semi-structured or structured nodes. |
| **11.** | **Fallback Model(s)** | Sometimes the retrieved content is insufficient, or the LLM fails; you need fallback mechanisms (like pattern based, simpler models) so system doesn’t completely break. Without fallback, user gets nonsense or empty. | - Detect low confidence; when confidence low, delegate to simpler model / default answer. <br>- Use ensemble of models or multiple LLMs. <br>- Provide a “no answer” or “I don’t know” fallback rather than hallucinating. <br>- Possibly hybrid models: retrieval + generation + rules. |
| **12.** | **LLM Security** | Risk of leaking sensitive data, prompt injection attacks, privacy issues, misuse. Also risk that retrieved documents have insecure content. | - Use access control / permission checks on who can retrieve what. <br>- Sanitize, filter out sensitive or PII data. <br>- Apply encryption wherever needed; log and audit retrievals. <br>- Prompt/security hardening: guard against prompt injection, misuse. <br>- Use secure environments / vet sources. |
